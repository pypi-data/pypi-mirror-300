Metadata-Version: 2.1
Name: loglu
Version: 0.1.0
Summary: The Logarithmic Linear Unit (LogLU) is a novel activation function designed for deep neural networks, improving convergence speed, stability, and overall model performance
Author: Rishi Chaitanya Sri Prasad Nalluri
Author-email: rishichaitanya888@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy >=1.20.0
Requires-Dist: tensorflow >=2.10.0
Requires-Dist: keras >=2.10.0


## LogLU (Logarithmic Linear Units) --- Activation Function

Welcome to **LogLU**, a novel activation function designed to take your deep neural networks 🚀

The **Logarithmic Linear Unit (LogLU)** improves convergence speed, stability, and overall model performance. Whether you're building AI for image recognition, NLP, or anything in between, this activation function is designed to make your models faster and smarter. 📈💡

### Why LogLU? 🤔

We all love activation functions like ReLU and its cousins, but sometimes, you need something a bit sharper and smarter. Here’s why LogLU stands out:

- ⚡ **Faster Convergence**: Your models train quicker, saving you time and resources.
- 💪 **Stability**: No more exploding or vanishing gradients—LogLU keeps things smooth.
- 📊 **Performance**: Whether it's accuracy or loss reduction, LogLU consistently outperforms traditional activations.

### Installation 💻

Ready to give it a go? Just run the following command to install LogLU:

```bash
pip install loglu
```

### Usage 🔧

Here’s how you can integrate LogLU into your deep learning models.

#### With TensorFlow:

```python
import tensorflow as tf
from loglu import LogLU 

# Define a model 
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, input_shape=(784,)),
    tf.keras.layers.Activation(LogLU()),  # Use LogLU as the activation function
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()
```

#### How It Works 🛠️

LogLU combines the smooth nature of logarithmic functions with the linear simplicity of ReLU. It’s designed to maintain gradient flow even in deep networks, ensuring fast and stable learning without the common pitfalls of gradient vanishing or exploding.

For all you math enthusiasts out there, here’s the equation:
- If `x > 0`: LogLU behaves like ReLU.
- If `x <= 0`: It has a logarithmic form, helping with negative values.

### Feedback, Bugs, & Contributions 🐛

Got feedback or found a bug? Feel free to open an issue or contribute on GitHub:

- GitHub: https://github.com/Rishichaitanya-Nalluri/LogLU
- Email: rishichaitanya888@gmail.com

Let’s make machine learning even more fun and efficient together!

---

Happy coding and may your models always converge faster than your coffee cools down! ☕


## Patent Notice

This software implements methods covered by Indian Published Patent Application No. 202341087256, currently published but not yet granted. Use of the patented methods may require a separate license. Please refer to the LICENSE file for more details.
