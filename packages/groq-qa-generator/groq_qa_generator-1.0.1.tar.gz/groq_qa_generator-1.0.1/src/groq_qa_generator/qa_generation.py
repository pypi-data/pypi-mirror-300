import logging
from .logging_setup import initialize_logging
from .groq_api import (
    get_api_key,
    get_groq_client,
    get_groq_completion,
    stream_completion,
)
from .text_processing import write_response_to_file
from .config import args


def load_sample_question(file_path):
    """Load the sample question from a file.

    Args:
        file_path (str): The path to the file containing the sample question.

    Returns:
        str: The content of the sample question file, stripped of leading/trailing whitespace.
    """
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()


def load_system_prompt(file_path, chunk_size, tokens_per_question):
    """Load and prepare the system prompt, adjusting it based on chunk size and tokens per question.

    This function reads the system prompt from a file and calculates how many questions should
    be generated per chunk based on the chunk size and tokens per question. It then replaces
    a placeholder ("<n>") in the system prompt with the calculated number of questions.

    Args:
        file_path (str): The path to the system prompt file.
        chunk_size (int): The number of tokens per chunk (default: 512).
        tokens_per_question (int): The number of tokens allocated for each question (default: 60).

    Returns:
        str: The formatted system prompt with the number of questions inserted.

    Notes:
        - With a default chunk_size of 512 and tokens_per_question of 60, this function
          will generate 8 questions per chunk (512 / 60 â‰ˆ 8).
    """
    questions_per_chunk = int(chunk_size / tokens_per_question)

    with open(file_path, "r", encoding="utf-8") as f:
        system_prompt = f.read().strip()

    system_prompt = system_prompt.replace("<n>", str(questions_per_chunk))

    return system_prompt


def create_groq_prompt(system_prompt, sample_question):
    """Combine the system prompt and sample question into a full prompt for Groq API.

    Args:
        system_prompt (str): The system prompt defining the behavior and tone of the model.
        sample_question (str): A sample question to help guide the generation.

    Returns:
        str: The full combined prompt including both the system prompt and sample question.
    """
    full_prompt = f"{system_prompt}\n\n{sample_question}"

    return full_prompt


def generate_qa_pairs(text_chunks, groq_config):
    """
    Generate question-answer pairs from text chunks using the Groq API.

    This function processes a list of text chunks and generates question-answer (QA) pairs based on
    the provided configuration. It leverages the Groq API for completion and logs the output to an
    output file, either in JSON or plain text format.

    Args:
        text_chunks (list of str): The list of text chunks to generate questions from.
        groq_config (dict): Configuration settings containing:
            - api_key (str): API key for accessing Groq.
            - system_prompt (str): Path to the system prompt file.
            - sample_question (str): Path to the sample question file.
            - chunk_size (int): Number of tokens per text chunk.
            - tokens_per_question (int): Number of tokens allocated for each question.
            - model (str): Model name or identifier for QA generation.
            - temperature (float): Temperature setting to control randomness in the model's output.
            - max_tokens (int): Maximum number of tokens in the response.
            - output_file (str): Path to the output file where results will be written.

    Returns:
        list of dict: A list of dictionaries containing generated question-answer pairs.

    Logs:
        Logs success and failure for each text chunk processed.
    """

    def process_chunk(
        client,
        chunk_index,
        total_chunks,
        chunk_text,
        system_prompt,
        sample_question,
        groq_config,
        all_qa_pairs,
    ):
        """
        Process a single chunk of text and generate QA pairs.

        This function handles the processing of a single text chunk, generating QA pairs based on
        the system prompt and sample question. It logs the progress and captures any errors encountered
        during processing.

        Args:
            client (object): The Groq client used for generating completions.
            chunk_index (int): The index of the current text chunk being processed.
            total_chunks (int): The total number of text chunks to be processed.
            chunk_text (str): The text chunk to process.
            system_prompt (str): The system prompt to be used for generating QA pairs.
            sample_question (str): The sample question to be used for generating QA pairs.
            groq_config (dict): The configuration settings including model, temperature, and max tokens.
            all_qa_pairs (list): A list to store all generated QA pairs.

        Side Effects:
            Updates the `all_qa_pairs` list with new QA pairs from the current chunk.
        """
        logging.info(f"Processing chunk {chunk_index + 1} of {total_chunks}")

        full_prompt_text = create_groq_prompt(system_prompt, sample_question)
        completion = get_groq_completion(
            client,
            full_prompt_text,
            chunk_text,
            groq_config["model"],
            groq_config["temperature"],
            groq_config["max_tokens"],
        )

        if completion:
            response = stream_completion(completion)
            logging.info(response)
            qa_pairs = parse_qa_pairs(response)
            all_qa_pairs.extend(qa_pairs)
        else:
            logging.error(f"Failed to generate QA pairs for chunk {chunk_index}.")

    def parse_qa_pairs(response):
        """
        Parse the model's response into question-answer pairs.

        This function splits the response from the model into individual question-answer
        pairs based on double newline characters.

        Args:
            response (str): The raw response text generated by the model.

        Returns:
            list of str: A list of question-answer pairs.
        """
        return response.strip().split("\n\n")

    def write_qa_pairs_to_file(qa_pairs, output_file, as_json):
        """
        Write the QA pairs to the specified output file, in JSON format if required.

        This function writes the accumulated QA pairs to the output file. If the `as_json`
        flag is set, the output will be written in JSON format; otherwise, it will be plain text.

        Args:
            qa_pairs (list of str): The list of question-answer pairs to be written to the file.
            output_file (str): The path to the output file where the QA pairs will be saved.
            as_json (bool): Whether to save the QA pairs in JSON format.

        Side Effects:
            Writes the QA pairs to the specified output file.
        """
        content = "\n\n".join(qa_pairs)
        write_response_to_file(content, output_file, as_json)

    api_key = get_api_key()
    client = get_groq_client(api_key)

    system_prompt = load_system_prompt(
        groq_config["system_prompt"],
        groq_config["chunk_size"],
        groq_config["tokens_per_question"],
    )

    sample_question = load_sample_question(groq_config["sample_question"])

    all_qa_pairs = []
    total_chunks = len(text_chunks)

    for chunk_index, chunk_text in enumerate(text_chunks):
        process_chunk(
            client,
            chunk_index,
            total_chunks,
            chunk_text,
            system_prompt,
            sample_question,
            groq_config,
            all_qa_pairs,
        )

    write_qa_pairs_to_file(all_qa_pairs, groq_config["output_file"], args.json)

    logging.info(f"QA pairs written to file: {groq_config['output_file']}")

    return all_qa_pairs
